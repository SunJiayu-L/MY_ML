
import numpy as np
np.set_printoptions(precision=2)
import matplotlib.pyplot as plt
dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'
plt.style.use('./deeplearning.mplstyle')
from lab_utils_multi import  load_house_data, compute_cost, run_gradient_descent
from lab_utils_multi import  norm_plot, plt_contour_multi, plt_equal_scale, plot_cost_i_w
from sklearn.preprocessing import scale

X_train, y_train = load_house_data()
X_features = ['size(sqft)','bedrooms','floors','age']

# è®©æˆ‘ä»¬é€šè¿‡ç»˜åˆ¶æ¯ä¸ªç‰¹å¾ä¸ä»·æ ¼çš„å…³ç³»æ¥æŸ¥çœ‹æ•°æ®é›†åŠå…¶ç‰¹å¾ã€‚
# ç»˜åˆ¶æ¯ä¸ªåŠŸèƒ½ä¸ç›®æ ‡ä»·æ ¼çš„å…³ç³»å›¾ï¼Œå¯ä»¥æä¾›ä¸€äº›å…³äºå“ªäº›åŠŸèƒ½å¯¹ä»·æ ¼å½±å“æœ€å¤§çš„æŒ‡ç¤ºã€‚
# å¦‚ä¸Šæ‰€è¿°ï¼Œå°ºå¯¸çš„å¢åŠ ä¹Ÿä¼šå¯¼è‡´ä»·æ ¼çš„ä¸Šæ¶¨ã€‚å§å®¤å’Œåœ°æ¿ä¼¼ä¹å¯¹ä»·æ ¼æ²¡æœ‰å¤ªå¤§å½±å“ã€‚æ–°æˆ¿å­çš„ä»·æ ¼æ¯”æ—§æˆ¿å­é«˜ã€‚
fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)
for i in range(len(ax)):
    ax[i].scatter(X_train[:,i],y_train)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("Price (1000's)")
plt.show()

# æ¢è®¨å­¦ä¹ ç‡ğ›¼å¯¹æ”¶æ•›çš„å½±å“
_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)
plot_cost_i_w(X_train, y_train, hist)

_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)
plot_cost_i_w(X_train, y_train, hist)

_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)
plot_cost_i_w(X_train, y_train, hist)

#
def zscore_normalize_features(X):
    """
    computes  X, zcore normalized by column

    Args:
      X (ndarray): Shape (m,n) input data, m examples, n features

    Returns:
      X_norm (ndarray): Shape (m,n)  input normalized by column     X_norm æ˜¯é€šè¿‡ zscore_normalize_features å‡½æ•°å¤„ç†åçš„æ•°æ®é›†
      mu (ndarray):     Shape (n,)   mean of each featureæ¯ä¸ªç‰¹å¾çš„å‡å€¼
      sigma (ndarray):  Shape (n,)   standard deviation of each featureæ¯ä¸ªç‰¹å¾çš„æ ‡å‡†å·®
    åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼ŒX_norm çš„è®¡ç®—æ­¥éª¤å¦‚ä¸‹ï¼š
    é¦–å…ˆï¼Œè®¡ç®—è¾“å…¥æ•°æ® X ä¸­æ¯ä¸ªç‰¹å¾ï¼ˆåˆ—ï¼‰çš„å‡å€¼ muã€‚
    ç„¶åï¼Œè®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†å·® sigmaã€‚
    å¯¹äº X ä¸­çš„æ¯ä¸ªæ ·æœ¬ï¼Œä»å…¶æ¯ä¸ªç‰¹å¾å€¼ä¸­å‡å»è¯¥ç‰¹å¾çš„å‡å€¼ï¼Œç„¶åé™¤ä»¥è¯¥ç‰¹å¾çš„æ ‡å‡†å·®ã€‚è¿™ä¸ªæ“ä½œæ˜¯é€å…ƒç´ ï¼ˆelement-wiseï¼‰è¿›è¡Œçš„ã€‚
    æœ€ç»ˆå¾—åˆ°çš„ X_norm æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸ X ç›¸åŒçš„æ•°ç»„ï¼Œä½†å…¶ç‰¹å¾å€¼å·²ç»æ ‡å‡†åŒ–ã€‚
    """

    # find the mean of each column/feature
    mu = np.mean(X, axis=0)  # mu will have shape (n,)
    # find the standard deviation of each column/feature
    sigma = np.std(X, axis=0)  # sigma will have shape (n,)
    # element-wise, subtract mu for that column from each example, divide by std for that column
    X_norm = (X - mu) / sigma

    return (X_norm, mu, sigma)


mu     = np.mean(X_train,axis=0)
sigma  = np.std(X_train,axis=0)
X_mean = (X_train - mu)
X_norm = (X_train - mu)/sigma

""""
å·¦ï¼šæœªæ ‡å‡†åŒ–ï¼šâ€œå°ºå¯¸ï¼ˆå¹³æ–¹è‹±å°ºï¼‰â€ç‰¹å¾çš„å€¼èŒƒå›´æˆ–æ–¹å·®è¿œå¤§äºå¹´é¾„çš„èŒƒå›´
ä¸­ï¼šç¬¬ä¸€æ­¥æŸ¥æ‰¾ä»æ¯ä¸ªç‰¹å¾ä¸­åˆ é™¤å¹³å‡å€¼ã€‚è¿™ç•™ä¸‹äº†ä»¥é›¶ä¸ºä¸­å¿ƒçš„ç‰¹å¾ã€‚å¾ˆéš¾çœ‹å‡ºâ€œå¹´é¾„â€ç‰¹å¾çš„å·®å¼‚ï¼Œä½†â€œå°ºå¯¸ï¼ˆå¹³æ–¹è‹±å°ºï¼‰â€æ˜¾ç„¶åœ¨é›¶å·¦å³ã€‚
å³ï¼šç¬¬äºŒæ­¥é™¤ä»¥æ–¹å·®ã€‚è¿™ä½¿å¾—ä¸¤ä¸ªç‰¹å¾éƒ½ä»¥é›¶ä¸ºä¸­å¿ƒï¼Œå…·æœ‰ç›¸ä¼¼çš„å°ºåº¦ã€‚
"""
#è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ Z-scoreæ ‡å‡†åŒ–æ‰€æ¶‰åŠçš„æ­¥éª¤ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†é€æ­¥çš„è½¬å˜ã€‚
fig,ax=plt.subplots(1, 3, figsize=(12, 3))
ax[0].scatter(X_train[:,0], X_train[:,3])
ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3])
ax[0].set_title("unnormalized")
ax[0].axis('equal')

ax[1].scatter(X_mean[:,0], X_mean[:,3])
ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3])
ax[1].set_title(r"X - $\mu$")
ax[1].axis('equal')

ax[2].scatter(X_norm[:,0], X_norm[:,3])
ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);
ax[2].set_title(r"Z-score normalized")
ax[2].axis('equal')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
fig.suptitle("distribution of features before, during, after normalization")
plt.show()

#è®©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¹¶å°†å…¶ä¸åŸå§‹æ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚
#é€šè¿‡å½’ä¸€åŒ–ï¼Œæ¯åˆ—çš„å³°å³°å€¼èŒƒå›´ä»æ•°åƒå€å‡å°‘åˆ° 2-3 å€ã€‚
X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)
print(f"X_mu = {X_mu}, \nX_sigma = {X_sigma}")
print(f"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}")
print(f"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}")

#è¯·æ³¨æ„ï¼Œä¸Šé¢æ ‡å‡†åŒ–æ•°æ®çš„èŒƒå›´ä»¥é›¶ä¸ºä¸­å¿ƒï¼Œå¤§è‡´ä¸º+/- 1ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæ¯ä¸ªç‰¹å¾çš„èŒƒå›´éƒ½æ˜¯ç›¸ä¼¼çš„ã€‚
fig,ax=plt.subplots(1, 4, figsize=(12, 3))
for i in range(len(ax)):
    norm_plot(ax[i],X_train[:,i],)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("count");
fig.suptitle("distribution of features before normalization")
plt.show()
fig,ax=plt.subplots(1,4,figsize=(12,3))
for i in range(len(ax)):
    norm_plot(ax[i],X_norm[:,i],)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("count");
fig.suptitle(f"distribution of features after normalization")

plt.show()

#ç¼©æ”¾åçš„ç‰¹å¾å¯ä»¥æ›´å¿«åœ°è·å¾—éå¸¸å‡†ç¡®çš„ç»“æœï¼è¯·æ³¨æ„ï¼Œåœ¨è¿™ä¸ªç›¸å½“çŸ­çš„è¿è¡Œç»“æŸæ—¶ï¼Œæ¯ä¸ªå‚æ•°çš„æ¢¯åº¦éƒ½å¾ˆå°ã€‚ 0.1 çš„å­¦ä¹ ç‡æ˜¯ä½¿ç”¨å½’ä¸€åŒ–ç‰¹å¾è¿›è¡Œå›å½’çš„è‰¯å¥½å¼€ç«¯ã€‚
w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, )

#è®©æˆ‘ä»¬ç»˜åˆ¶é¢„æµ‹å€¼ä¸ç›®æ ‡å€¼çš„å…³ç³»å›¾ã€‚è¯·æ³¨æ„ï¼Œé¢„æµ‹æ˜¯ä½¿ç”¨å½’ä¸€åŒ–ç‰¹å¾è¿›è¡Œçš„ï¼Œè€Œç»˜å›¾æ˜¯ä½¿ç”¨åŸå§‹ç‰¹å¾å€¼æ˜¾ç¤ºçš„
#predict target using normalized features
m = X_norm.shape[0]
yp = np.zeros(m)
for i in range(m):
    yp[i] = np.dot(X_norm[i], w_norm) + b_norm

    # plot predictions and targets versus original features
fig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)
for i in range(len(ax)):
    ax[i].scatter(X_train[:,i],y_train, label = 'target')
    ax[i].set_xlabel(X_features[i])
    ax[i].scatter(X_train[:,i],yp,color=dlorange, label = 'predict')
ax[0].set_ylabel("Price"); ax[0].legend()
fig.suptitle("target versus prediction using z-score normalized model")
plt.show()

# First, normalize out example.
x_house = np.array([1200, 3, 1, 40])
x_house_norm = (x_house - X_mu) / X_sigma
print(x_house_norm)
x_house_predict = np.dot(x_house_norm, w_norm) + b_norm
print(f" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}")

#åœ¨ä¸‹å›¾ä¸­ï¼Œå‚æ•°çš„æ¯”ä¾‹æ˜¯åŒ¹é…çš„ã€‚å·¦å›¾æ˜¯ w[0]ï¼ˆå¹³æ–¹è‹±å°ºï¼‰ä¸ w[1]ï¼ˆæ ‡å‡†åŒ–ç‰¹å¾ä¹‹å‰çš„å§å®¤æ•°é‡ï¼‰çš„æˆæœ¬ç­‰å€¼çº¿å›¾ã€‚
# è¯¥å›¾éå¸¸ä¸å¯¹ç§°ï¼Œä»¥è‡³äºçœ‹ä¸åˆ°å®Œæˆè½®å»“çš„æ›²çº¿ã€‚ç›¸åï¼Œå½“ç‰¹å¾æ ‡å‡†åŒ–æ—¶ï¼Œæˆæœ¬è½®å»“æ›´åŠ å¯¹ç§°ã€‚ç»“æœæ˜¯ï¼Œåœ¨æ¢¯åº¦ä¸‹é™æœŸé—´æ›´æ–°å‚æ•°å¯ä»¥ä½¿æ¯ä¸ªå‚æ•°å–å¾—ç›¸åŒçš„è¿›å±•ã€‚
plt_equal_scale(X_train, X_norm, y_train)