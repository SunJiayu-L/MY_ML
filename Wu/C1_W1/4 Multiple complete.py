#è¿™ä¸ªä»£ç åŠ å…¥äº†ä¹‹å‰çš„ä¸€äº›ä¸œè¥¿ å›¾å˜å¤šäº†è€Œå·²  ä¸»è¦è¿˜æ˜¯çœ‹4 Multiple Variable Linear Regression
import copy
import math
import numpy as np
import matplotlib.pyplot as plt
from lab_utils_uni import plt_gradients, plt_contour_wgrad, plt_divergence  # ç¡®ä¿è¿™äº›å‡½æ•°å­˜åœ¨äºæ‚¨çš„é¡¹ç›®ä¸­
plt.style.use('./deeplearning.mplstyle')


#å¼•å…¥æ•°æ® #  a house with 1000 square feet sold for $300,000
#           and a house with 2000 square feet sold for $500,000.
x_train = np.array([1.0, 2.0])
y_train = np.array([300.0, 500.0])


#è®¡ç®—æˆæœ¬å‡½æ•°ï¼Œéå†æ¯ä¸€ä¸ªæ•°æ®
def cost_function(x,y,w,b):
    m=len(x)
    cost_sum=0
    for i in range(m):
        f_wb=w*x[i]+b
        cost=(f_wb-y[i])**2
        cost_sum=cost_sum+cost
    total_cost = (1 / (2 * m)) * cost_sum

    return  total_cost

#f_dw å³ä¸ºy_hat
#è®¡ç®—åå¯¼æ•°ï¼Œè¾“å…¥å€¼ä¸ºxè®­ç»ƒé›†ï¼Œyç›®æ ‡é›†ï¼Œwbä¸ºå‡è®¾æ–œç‡ä¸æˆªè·ï¼Œè¿”å›åå¯¼æ•°dj_dw,dj_db
def computer_gradient(x,y,w,b):
    m=len(x)
    dj_dw=0
    dj_db=0
    for i in range(m):
        f_wb=w*x[i]+b
        dj_dw_i=(f_wb-y[i])*x[i]
        dj_db_i=f_wb-y[i]
        dj_db += dj_db_i
        dj_dw += dj_dw_i
    dj_dw = dj_dw / m
    dj_db = dj_db / m

    return  dj_dw,dj_db

#å·¦ä¸€æ˜¾ç¤ºäº†  âˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘¤æˆæœ¬æ›²çº¿  ğ‘¤ç›¸å¯¹äºä¸‰ä¸ªç‚¹çš„æ–œç‡ã€‚åœ¨å›¾çš„å³ä¾§ï¼Œå¯¼æ•°ä¸ºæ­£æ•°ï¼Œè€Œåœ¨å·¦ä¾§ä¸ºè´Ÿæ•°ã€‚
# ç”±äºâ€œç¢—å½¢â€ï¼Œå¯¼æ•°å°†å§‹ç»ˆå¯¼è‡´æ¢¯åº¦ä¸‹é™åˆ°æ¢¯åº¦ä¸ºé›¶çš„åº•éƒ¨ã€‚
#å³è¾¹çš„å›¾å·²ç»å›ºå®š  ğ‘=100äº†ã€‚æ¢¯åº¦ä¸‹é™å°†åŒæ—¶åˆ©ç”¨ä¸¤è€…  âˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘¤ï¼Œâˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘æ¥æ›´æ–°å‚æ•°ã€‚
#å³ä¾§çš„â€œç®­è¢‹å›¾â€æä¾›äº†ä¸€ç§æŸ¥çœ‹ä¸¤ä¸ªå‚æ•°æ¢¯åº¦çš„æ–¹æ³•ã€‚ç®­å¤´å¤§å°åæ˜ äº†è¯¥ç‚¹çš„æ¢¯åº¦å¤§å°ã€‚ç®­å¤´çš„æ–¹å‘å’Œæ–œç‡åæ˜ äº†è¯¥ç‚¹çš„  âˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘¤æ¯”ç‡  âˆ‚ğ½(ğ‘¤,ğ‘)âˆ‚ğ‘ã€‚
#è¯·æ³¨æ„ï¼Œæ¢¯åº¦ç‚¹è¿œç¦»æœ€å°å€¼ã€‚ç¼©æ”¾çš„æ¢¯åº¦ä»ğ‘¤æˆ–ğ‘çš„å½“å‰å€¼ ä¸­å‡å»ã€‚è¿™ä¼šå°†å‚æ•°å‘é™ä½æˆæœ¬çš„æ–¹å‘ç§»åŠ¨ã€‚
plt_gradients(x_train,y_train, cost_function, computer_gradient)

plt.show()

"""
Performs gradient descent to fit w,b. Updates w,b by taking 
num_iters gradient steps with learning rate alpha

Args:
  x (ndarray (m,))  : Data, m examples 
  y (ndarray (m,))  : target values
  w_in,b_in (scalar): initial values of model parameters  
  alpha (float):     Learning rate
  num_iters (int):   number of iterations to run gradient descent
  cost_function:     function to call to produce cost
  gradient_function: function to call to produce gradient

Returns:
  w (scalar): Updated value of parameter after running gradient descent
  b (scalar): Updated value of parameter after running gradient descent
  J_history (List): History of cost values
  p_history (list): History of parameters [w,b] 
  """
def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):
    w = copy.deepcopy(w_in)  # avoid modifying global w_in
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    p_history = []
    b = b_in
    w = w_in

    for i in range(num_iters):
        # è·å¾—åå¯¼æ•°
        dj_dw, dj_db = gradient_function(x, y, w, b)

        # æ›´æ–°å‚æ•°
        b = b - alpha * dj_db
        w = w - alpha * dj_dw

        # Save cost J at each iterationä¿å­˜æ¯æ¬¡è¿­ä»£å®ŒJçš„æ•°æ®
        if i < 100000:  # prevent resource exhaustion ä¸ºäº†é¿å…èµ„æºè€—å°½ï¼ˆä¾‹å¦‚å†…å­˜æˆ–è®¡ç®—èµ„æºï¼‰ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•å¯èƒ½ä¼šè¿è¡Œå¾ˆå¤šæ¬¡è¿­ä»£ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æˆ–è€…å¤æ‚çš„æ¨¡å‹æ—¶ã€‚
                    # å¦‚æœæ¯æ¬¡éƒ½è®°å½•å’Œæ‰“å°æˆæœ¬å‡½æ•°çš„å€¼ï¼Œå¯èƒ½ä¼šäº§ç”Ÿå¤§é‡çš„æ•°æ®ï¼Œè¿™ä¸ä»…ä¼šå ç”¨å¤§é‡çš„å†…å­˜ç©ºé—´ï¼Œè¿˜å¯èƒ½å¯¼è‡´ç¨‹åºè¿è¡Œç¼“æ…¢ã€‚
                    # å› æ­¤ï¼Œé€šè¿‡è®¾ç½®ä¸€ä¸ªåˆç†çš„è¿­ä»£æ¬¡æ•°ä¸Šé™ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶èµ„æºçš„ä½¿ç”¨ï¼Œé˜²æ­¢ç¨‹åºå› ä¸ºèµ„æºä¸è¶³è€Œå´©æºƒã€‚
            J_history.append(cost_function(x, y, w, b))#ä¿å­˜æˆæœ¬å‡½æ•°J
            p_history.append([w, b])#ä¿å­˜æ¯ä¸€æ¬¡çš„wb
        # Print cost every at intervals 10 times or as many iterations if < 10
        if i % math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
                f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
                f"w: {w: 0.3e}, b:{b: 0.5e}")
    return w, b, J_history, p_history  # return w and J,w history for graphing

#å‡½æ•°å®ç°
# initialize parameters
w_init = 0
b_init = 0
# some gradient descent settingsï¼ˆè¿­ä»£æ¬¡æ•°ä¸å­¦ä¹ ç‡ï¼‰
iterations = 10000
tmp_alpha = 1.0e-2
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,
                                                    iterations, cost_function, computer_gradient)
print(f"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})")

#ç»˜å›¾
# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')
# Set the title
plt.title("Housing Prices")
# Set the y-axis label
plt.ylabel('Price (in 1000s of dollars)')
# Set the x-axis label
plt.xlabel('Size (1000 sqft)')
plt.legend()
plt.plot(x_train, w_final*x_train+b_final, c='b',label='Our Prediction')
plt.show()

# æ¢¯åº¦ä¸‹é™çš„æˆæœ¬ä¸è¿­ä»£æ¬¡æ•°ï¼šåˆšå¼€å§‹æ—¶ï¼Œå°‘é‡è¿­ä»£å¯ä»¥ä½¿æˆæœ¬å‡½æ•°éª¤é™ï¼Œç»“æŸæ—¶ï¼Œå¤§é‡è¿­ä»£å¸¦æ¥çš„æˆæœ¬å‡½æ•°å˜åŒ–éå¸¸å°

fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))
ax1.plot(J_hist[:100])
ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])
ax1.set_title("Cost vs. iteration(start)");  ax2.set_title("Cost vs. iteration (end)")
ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')
ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')
plt.show()


print(f"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars")
print(f"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars")
print(f"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars")

fig1, ax = plt.subplots(1,1, figsize=(12, 6))
plt_contour_wgrad(x_train, y_train, p_hist, ax)
plt.show()

fig2, ax = plt.subplots(1,1, figsize=(12, 4))
plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],
            contours=[1,5,10,20],resolution=0.5)
plt.show()


# initialize parameters
w_init = 0
b_init = 0
# set alpha to a large value
iterations = 10
tmp_alpha = 8.0e-1
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,
                                                    iterations, cost_function, computer_gradient)
plt_divergence(p_hist, J_hist,x_train, y_train)
plt.show()
